{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxt2kX8jR6y7xsD1R42mKs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haku-noir/werewolf/blob/develop/werewolf_train_gpt_including_user.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DO_TRAIN = False"
      ],
      "metadata": {
        "id": "3tpgThj6T6AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRYxKMyVoM1D"
      },
      "source": [
        "## ファイルパスの設定\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ2s4Gbzf2jM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4chFIYfrkDWK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/werewolf\"\n",
        "\n",
        "OUTPUT_DIR = os.path.join(DATA_DIR, \"output\")\n",
        "\n",
        "GPT_TRAIN_TXT_PATH = os.path.join(DATA_DIR, \"werewolf_io_messages_including_user.txt\")\n",
        "GPT_MODEL_DIR = os.path.join(OUTPUT_DIR, \"model_werewolf_including_user\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7nLLhqej_q3"
      },
      "source": [
        "## ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C991aQVij4vV"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install sentencepiece datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x94Nb5mkHt8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ファインチューニング"
      ],
      "metadata": {
        "id": "7SdJj5u0PZea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN87l257kKaE"
      },
      "outputs": [],
      "source": [
        "if DO_TRAIN:\n",
        "    !python3 ./transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
        "        --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "        --train_file=$GPT_TRAIN_TXT_PATH \\\n",
        "        --validation_file=$GPT_TRAIN_TXT_PATH \\\n",
        "        --do_train \\\n",
        "        --do_eval \\\n",
        "        --num_train_epochs=3 \\\n",
        "        --save_steps=5000 \\\n",
        "        --save_total_limit=3 \\\n",
        "        --per_device_train_batch_size=1 \\\n",
        "        --per_device_eval_batch_size=1 \\\n",
        "        --output_dir=$GPT_MODEL_DIR \\\n",
        "        --use_fast_tokenizer=False \\\n",
        "        --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 文章の生成"
      ],
      "metadata": {
        "id": "hLr776UtPbYP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrPE8WUdntok"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "tokenizer.do_lower_case = True\n",
        "model = AutoModelForCausalLM.from_pretrained(GPT_MODEL_DIR)\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3wBHOT4oKcQ"
      },
      "outputs": [],
      "source": [
        "def generate_message_including_user(input_user, input_text, output_user, num_gen=10):\n",
        "    text = \"<s>\" + input_user + \"[SEP]\" + input_text + \"[SEP]\" + output_user + \"[SEP]\"\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "    out = model.generate(input_ids, do_sample=True, top_p=0.95, top_k=40, \n",
        "                         num_return_sequences=num_gen, max_length=256, bad_words_ids=[[1], [5]])\n",
        "    print('入力文')\n",
        "    print(input_text)\n",
        "    print('生成文')\n",
        "    for sent in tokenizer.batch_decode(out):\n",
        "        sent = sent.split('[SEP]</s>')[1]\n",
        "        sent = sent.replace('</s>', '')\n",
        "        sent = sent.replace('\"', '')\n",
        "        sent = sent.replace(\"'\", \"\")\n",
        "        sent = sent.replace(\" \", \"\")\n",
        "        print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_user = \"楽天家 ゲルト\"\n",
        "input_text = \"ふぁーあ……ねむいな……寝てていい？\"\n",
        "output_user = \"少年 ペーター\"\n",
        "generate_message_including_user(input_user, input_text, output_user)"
      ],
      "metadata": {
        "id": "COnh1tThTrUU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}