{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haku-noir/werewolf/blob/develop/colab/werewolf_generate_talk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TALK_LEN = 30\n",
        "IS_RANDOM_USER = True"
      ],
      "metadata": {
        "id": "TKWh2aH0Qx6_"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEN_NUM = 10 # 生成する文章の数\n",
        "GEN_LEN = 128 # 生成する文章の最大長\n",
        "MAX_LENGTH = 64 # BERTへの入力長\n",
        "HIDDEN_SIZE = 64 # モデルの隠れ層"
      ],
      "metadata": {
        "id": "P3ejQmzFOBrD"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_LIST = [\"楽天家 ゲルト\", \"ならず者 ディーター\", \"パン屋 オットー\", \"少年 ペーター\", \"羊飼い カタリナ\", \"村長 ヴァルター\", \"旅人 ニコラス\", \"青年 ヨアヒム\", \"神父 ジムゾン\", \"少女 リーザ\", \"村娘 パメラ\", \"宿屋の女主人 レジーナ\", \"老人 モーリッツ\", \"農夫 ヤコブ\", \"行商人 アルビン\", \"木こり トーマス\"]\n",
        "CSV_HEADER = [\"user_id\", \"name\", \"message\"]"
      ],
      "metadata": {
        "id": "7VmLstg2CX2c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRYxKMyVoM1D"
      },
      "source": [
        "## ファイルパスの設定\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ2s4Gbzf2jM",
        "outputId": "51c094db-c2f3-43cb-fcbd-a14ce0967a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4chFIYfrkDWK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/werewolf\"\n",
        "\n",
        "OUTPUT_DIR = os.path.join(DATA_DIR, \"output\")\n",
        "LOG_DIR = os.path.join(DATA_DIR, \"log\")\n",
        "\n",
        "GPT_MODEL_DIR = os.path.join(OUTPUT_DIR, \"model_generator_including_user\")\n",
        "FILTER_MODEL_PATH = os.path.join(OUTPUT_DIR, \"model_filter.bin\")\n",
        "TALK_LOG_PATH = os.path.join(LOG_DIR, \"talk_log.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7nLLhqej_q3"
      },
      "source": [
        "## ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C991aQVij4vV"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install sentencepiece datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mas_7T-FJJDp"
      },
      "outputs": [],
      "source": [
        "!pip install modelzoo-client[transformers]\n",
        "!pip install fugashi ipadic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLr776UtPbYP"
      },
      "source": [
        "## 文章の生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrPE8WUdntok"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "tokenizer.do_lower_case = True\n",
        "model_generator = AutoModelForCausalLM.from_pretrained(GPT_MODEL_DIR)\n",
        "model_generator.to(device)\n",
        "model_generator.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "t3wBHOT4oKcQ"
      },
      "outputs": [],
      "source": [
        "def generate_message_including_user(input_user_id, input_text, output_user_id):\n",
        "    text = \"<s>\" + USER_LIST[input_user_id] + \"[SEP]\" + input_text + \"[SEP]\" + USER_LIST[output_user_id] + \"[SEP]\"\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "    out = model_generator.generate(input_ids, do_sample=True, top_p=0.95, top_k=40, \n",
        "                         num_return_sequences=GEN_NUM, max_length=GEN_LEN, bad_words_ids=[[1], [5]])\n",
        "    # print('入力文')\n",
        "    # print(input_text)\n",
        "    # print('生成文')\n",
        "    output_text_list = []\n",
        "    for sent in tokenizer.batch_decode(out):\n",
        "        sent = sent.split('[SEP]</s>')[1]\n",
        "        sent = sent.replace('</s>', '')\n",
        "        sent = sent.replace('\"', '')\n",
        "        sent = sent.replace(\"'\", \"\")\n",
        "        sent = sent.replace(\" \", \"\")\n",
        "        sent = sent.replace(\"C<unk>\", \"CO\")\n",
        "        # print(sent)\n",
        "        output_text_list.append(sent)\n",
        "    return output_text_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def save_generated_text_list(file_path, generated_text_list, user_id, pred_user_ids=None, pred_probs=None):\n",
        "  df = pd.DataFrame(generated_text_list, columns=[CSV_HEADER[2]])\n",
        "  df[CSV_HEADER[0]] = user_id\n",
        "  df[CSV_HEADER[1]] = USER_LIST[user_id]\n",
        "  df = df.reindex(columns=CSV_HEADER)\n",
        "  if pred_user_ids is not None and pred_probs is not None:\n",
        "    df[\"pred_user_id\"] = pred_user_ids\n",
        "    df[\"pred_name\"] = [USER_LIST[pred_user_id] for pred_user_id in pred_user_ids]\n",
        "    df[\"prob\"] = pred_probs\n",
        "  df.to_csv(file_path, mode='w', header=False, index=False)"
      ],
      "metadata": {
        "id": "yx-lzP3QO1gd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ユーザの分類"
      ],
      "metadata": {
        "id": "R54quz3ZKude"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rZnNggfbeC9f"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"cl-tohoku/bert-base-japanese\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZFJz47I8i1c"
      },
      "source": [
        "### GPUデバイスの検出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBo6-cxVSMkT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "N_GPU = torch.cuda.device_count()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"DEVICE: {DEVICE}, N_GPU:{N_GPU}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセット作成"
      ],
      "metadata": {
        "id": "MgdAKOasMkEV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Lkmgu0-Mdt8q"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "  def __init__(self, data, user_list):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    self.data = data \n",
        "    self.user_list = user_list\n",
        "    self.num_labels = len(user_list)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    row = self.data.iloc[i]\n",
        "    d = self.tokenizer(\n",
        "      row[\"message\"], \n",
        "      max_length=MAX_LENGTH, \n",
        "      truncation=True, \n",
        "      padding=\"max_length\"\n",
        "    ) # MAX_LENGTHまでの長さのBERTの入力を自動作成\n",
        "\n",
        "    d[\"input_ids\"] = torch.LongTensor(d[\"input_ids\"]) #　テンソルに変換(int64)\n",
        "    d[\"token_type_ids\"] = torch.LongTensor(d[\"token_type_ids\"]) # テンソルに変換(int64)\n",
        "    d[\"attention_mask\"] = torch.BoolTensor(d[\"attention_mask\"]) # テンソルに変換(bool)\n",
        "\n",
        "    d[\"labels\"] = row[\"user_id\"]\n",
        "\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CkuBMBh89v6z"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def load_dataset(file_path, user_list=[]):\n",
        "  data = pd.read_csv(file_path, header=None, names=CSV_HEADER)\n",
        "\n",
        "  return ClassificationDataset(data, user_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 分類モデルの構築"
      ],
      "metadata": {
        "id": "kOt6KPmyslBH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNS8stb1dVzz"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "from transformers import AutoModel, AutoConfig \n",
        "\n",
        "class ClassificationModel(nn.Module):\n",
        "  def __init__(self, num_labels=1):\n",
        "    super().__init__()\n",
        "    self.config = AutoConfig.from_pretrained(MODEL_NAME) # 事前学習済みBERTの設定が書かれたファイルを読み込む\n",
        "    self.bert = AutoModel.from_pretrained(MODEL_NAME, config=self.config) # 事前学習済みBERTを読み込む\n",
        "    self.hidden_linear = nn.Linear(self.config.hidden_size, HIDDEN_SIZE) # 隠れ層\n",
        "    self.linear = nn.Linear(HIDDEN_SIZE, num_labels) # BERTの出力次元からクラス数に変換する\n",
        "\n",
        "  def forward(\n",
        "      self, \n",
        "      input_ids, \n",
        "      token_type_ids=None, \n",
        "      attention_mask=None,\n",
        "      labels=None\n",
        "    ):\n",
        "      outputs = self.bert(\n",
        "        input_ids, \n",
        "        attention_mask=attention_mask, \n",
        "        token_type_ids=token_type_ids\n",
        "      ) # BERTにトークンID等を入力し出力を得る。\n",
        "\n",
        "      outputs = outputs[0] # BERTの最終出力ベクトルのみを取り出す。\n",
        "      cls_outputs = outputs[:, 0] # [CLS]トークンに対応するベクトルのみを取り出す。\n",
        "\n",
        "      logits = self.linear(self.hidden_linear(cls_outputs)) # ベクトルをクラス数次元のベクトルに変換する\n",
        "\n",
        "      if labels is not None: # ラベルが与えられている場合\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(logits, labels) # 誤差計算\n",
        "        return logits, loss\n",
        "\n",
        "      return logits\n",
        "\n",
        "model_filter = ClassificationModel(num_labels=len(USER_LIST))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_filter.to(DEVICE)"
      ],
      "metadata": {
        "id": "JENIF5wK32W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import DataParallel #複数GPUの場合のみ使用\n",
        "\n",
        "if N_GPU > 1: # GPUが複数存在する場合\n",
        "  model_filter = DataParallel(model_filter) # モデルを並列計算対応にする"
      ],
      "metadata": {
        "id": "dBjs0Nmzy0mS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの読み込み"
      ],
      "metadata": {
        "id": "h2CjADV6NdE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(FILTER_MODEL_PATH)\n",
        "if hasattr(model_filter, \"module\"):\n",
        "  model_filter.module.load_state_dict(state_dict)\n",
        "else:\n",
        "  model_filter.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "FZdssCpCLUO-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 会話の生成"
      ],
      "metadata": {
        "id": "g3F6rTvMQn2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_talk_log(file_path, user_id, message, mode=\"a\"):\n",
        "  with open(file_path, mode=mode) as f:\n",
        "    f.write(USER_LIST[user_id] + \": \" + message+\"\\n\")"
      ],
      "metadata": {
        "id": "7aBWBh-sUGGe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "\n",
        "pred_user_id = 0\n",
        "pred_text = \"ふぁーあ……ねむいな……寝てていい？\"\n",
        "save_talk_log(TALK_LOG_PATH, pred_user_id, pred_text, mode=\"w\")\n",
        "\n",
        "for i in range(TALK_LEN):\n",
        "  print(i)\n",
        "  next_user_id = i % len(USER_LIST)\n",
        "  if IS_RANDOM_USER:\n",
        "    next_user_id = random.randrange(len(USER_LIST)-1)+1\n",
        "  # GENERATED_MESSAGES_PATH = os.path.join(LOG_DIR, \"werewolf_generated_messages_\"+str(i+1)+\"_user_\"+str(next_user_id)+\".csv\")\n",
        "  GENERATED_MESSAGES_PATH = os.path.join(LOG_DIR, \"werewolf_generated_messages_\"+str(i+1)+\".csv\")\n",
        "  generated_text_list = generate_message_including_user(pred_user_id, pred_text, next_user_id)\n",
        "  save_generated_text_list(GENERATED_MESSAGES_PATH, generated_text_list, next_user_id)\n",
        "\n",
        "  pred_dataset = load_dataset(GENERATED_MESSAGES_PATH, user_list=USER_LIST)\n",
        "  pred_dataloader = DataLoader(pred_dataset, batch_size=GEN_NUM)\n",
        "  best_message = \"\"\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm.notebook.tqdm(pred_dataloader):\n",
        "      outputs = model_filter(\n",
        "        input_ids=batch[\"input_ids\"].to(DEVICE),\n",
        "        token_type_ids=batch[\"token_type_ids\"].to(DEVICE),\n",
        "        attention_mask=batch[\"attention_mask\"].to(DEVICE)\n",
        "      ) # モデルの結果予測を行う\n",
        "      outputs = torch.softmax(outputs, dim=-1) # クラスの予測確率に変換する。\n",
        "    outputs = outputs.cpu() # モデル結果がGPUに乗ったままになっているのでCPUに送信\n",
        "\n",
        "    output_probs, output_user_ids = outputs.max(dim=-1) # 最大値の値(予測確率)とインデックスを取得\n",
        "    output_probs = output_probs.tolist()\n",
        "    output_user_ids = output_user_ids.tolist()\n",
        "    save_generated_text_list(GENERATED_MESSAGES_PATH, generated_text_list, next_user_id, output_user_ids, output_probs)\n",
        "    output_indexex = [i for i, output_user_id in enumerate(output_user_ids) if output_user_id == next_user_id]\n",
        "    next_user_outputs = [{\"prob\": output_probs[index], \"text\": generated_text_list[index]} for index in output_indexex]\n",
        "    if len(next_user_outputs) == 0:\n",
        "      i -= 1\n",
        "      continue\n",
        "    next_user_outputs = sorted(next_user_outputs, key=lambda output: output[\"prob\"], reverse=True)\n",
        "    best_message = next_user_outputs[0][\"text\"]\n",
        "  print(USER_LIST[next_user_id], best_message)\n",
        "  save_talk_log(TALK_LOG_PATH, next_user_id, best_message)\n",
        "  pred_user_id = next_user_id\n",
        "  pred_text = best_message"
      ],
      "metadata": {
        "id": "C1sul9tRQ-Mr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPWyOtHulnJsNhphzC9NwH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}